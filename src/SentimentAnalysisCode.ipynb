{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Sentiment Analysis</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Datasets: ISEAR\n",
    "### 2. Features: Bag of Words of Unigram, Bigrams and Trigrams\n",
    "### 3. Classifiers: Naive Bayes Classifier, Maximum Entropy Classifier and Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Import Sklearn Libraries\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Import other modules\n",
    "import numpy as np # To Make the list and array computations easy\n",
    "import pandas # To read the data from the excel sheet\n",
    "import time # To keep track of the time taken in training and testing\n",
    "import math # To use the log function for maximum likelihood probabilities\n",
    "import random # To shuffle the data for different epochs\n",
    "import threading # To interact with GUI\n",
    "import SentimentGUI as gui # GUI class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "n = 1 ## Can be 1,2,3\n",
    "train_count = 6500\n",
    "test_count = 1000\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data from the excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    df = pandas.read_excel(filename)\n",
    "    values = df['SIT'].values\n",
    "    label=df['Field1'].values\n",
    "    return values.tolist(),label.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop-words\n",
    "{‘ourselves’, ‘hers’, ‘between’, ‘yourself’, ‘but’, ‘again’, ‘there’, ‘about’, ‘once’, ‘during’, ‘out’, ‘very’, ‘having’, ‘with’, ‘they’, ‘own’, ‘an’, ‘be’, ‘some’, ‘for’, ‘do’, ‘its’, ‘yours’, ‘such’, ‘into’, ‘of’, ‘most’, ‘itself’, ‘other’, ‘off’, ‘is’, ‘s’, ‘am’, ‘or’, ‘who’, ‘as’, ‘from’, ‘him’, ‘each’, ‘the’, ‘themselves’, ‘until’, ‘below’, ‘are’, ‘we’, ‘these’, ‘your’, ‘his’, ‘through’, ‘don’, ‘nor’, ‘me’, ‘were’, ‘her’, ‘more’, ‘himself’, ‘this’, ‘down’, ‘should’, ‘our’, ‘their’, ‘while’, ‘above’, ‘both’, ‘up’, ‘to’, ‘ours’, ‘had’, ‘she’, ‘all’, ‘no’, ‘when’, ‘at’, ‘any’, ‘before’, ‘them’, ‘same’, ‘and’, ‘been’, ‘have’, ‘in’, ‘will’, ‘on’, ‘does’, ‘yourselves’, ‘then’, ‘that’, ‘because’, ‘what’, ‘over’, ‘why’, ‘so’, ‘can’, ‘did’, ‘not’, ‘now’, ‘under’, ‘he’, ‘you’, ‘herself’, ‘has’, ‘just’, ‘where’, ‘too’, ‘only’, ‘myself’, ‘which’, ‘those’, ‘i’, ‘after’, ‘few’, ‘whom’, ‘t’, ‘being’, ‘if’, ‘theirs’, ‘my’, ‘against’, ‘a’, ‘by’, ‘doing’, ‘it’, ‘how’, ‘further’, ‘was’, ‘here’, ‘than’}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(tokens):\n",
    "    return [w for w in tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(tokens):\n",
    "    return [lmtzr.lemmatize(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_words(tokens):\n",
    "    return [ps.stem(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get n-grams word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens,n):\n",
    "    return [list(i)[0:n] for i in ngrams(tokens,n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return all the ngrams from unigram to n-gram\n",
    "def get_allngrams(tokens,n):\n",
    "    ngrams_list = []\n",
    "    for k in range(1,n+1):\n",
    "        ngrams_list.extend([list(i)[0:k] for i in ngrams(tokens,k)])\n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the words with length less than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_words(tokens):\n",
    "    new_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        if(len(tokens[i]) > 2):\n",
    "            new_tokens.append(tokens[i])\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(sentence_list,label_list):\n",
    "    sentence_count = len(sentence_list)\n",
    "    feature_list = []\n",
    "    vertical_stack = []\n",
    "    for i in range(sentence_count):\n",
    "        tokens = tokenize_sentence(sentence_list[i])\n",
    "        tokens = removeStopWords(tokens)\n",
    "        \n",
    "        ### Can choose option between stemming and lemmatization\n",
    "        #tokens = root_words(tokens)\n",
    "        tokens = lemmatize_words(tokens)\n",
    "        \n",
    "        tokens = remove_small_words(tokens)\n",
    "        \n",
    "        # All Combinations of n-grams\n",
    "        #ngram_list = get_allngrams(tokens,n)\n",
    "        ngram_list = get_ngrams(tokens,n)\n",
    "        new_num_list = []\n",
    "        for j in range(len(ngram_list)):\n",
    "            ngram_list[j].sort()\n",
    "            vertical_stack.extend(ngram_list[j])\n",
    "            new_num_list.extend(ngram_list[j])\n",
    "        feature_list.append(new_num_list)\n",
    "\n",
    "    return feature_list, vertical_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_entry(vertical_stack):\n",
    "    # #### Unique entries in the vertical stack\n",
    "    processed_set = []\n",
    "    for i in range(len(vertical_stack)):\n",
    "        if(not vertical_stack[i] in processed_set):\n",
    "            processed_set.append(vertical_stack[i])\n",
    "    return processed_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_features(feature_list, processed_set, list_class, label_list):\n",
    "    weighted_log_likelihood = np.array([[0.0 for column in range(len(processed_set))] for row in range(7)])\n",
    "    weighted_log_likelihood_score = np.array([[0.0 for column in range(len(processed_set))] for row in range(7)])\n",
    "\n",
    "    for i in range(len(feature_list)):\n",
    "        for j in range(len(feature_list[i])):\n",
    "            row = list_class.index(label_list[i])\n",
    "            col = processed_set.index(feature_list[i][j])\n",
    "            weighted_log_likelihood[row][col] += 1\n",
    "    \n",
    "    col_sum = np.sum(weighted_log_likelihood, axis = 0)\n",
    "    row_sum = np.sum(weighted_log_likelihood, axis = 1)\n",
    "    total_sum = np.sum(row_sum)\n",
    "    \n",
    "\n",
    "    for i in range(7):\n",
    "        for j in range(len(processed_set)):\n",
    "            p_w_c = weighted_log_likelihood[i][j] / row_sum[i]\n",
    "            p_w_notc = (col_sum[j] - weighted_log_likelihood[i][j]) / (total_sum - row_sum[i])\n",
    "            \n",
    "            ratio = 1\n",
    "            if(p_w_notc == 0):\n",
    "                ratio = 0\n",
    "            else:\n",
    "                ratio = p_w_c / p_w_notc\n",
    "            \n",
    "            # print(weighted_log_likelihood[i][j],row_sum[i], p_w_c, p_w_notc, ratio)\n",
    "            # If word doesn't exist in other classes\n",
    "            if(p_w_notc == 0):\n",
    "                weighted_log_likelihood_score[i][j] = p_w_c * 10\n",
    "            elif(ratio > 0):\n",
    "                value = p_w_c * math.log(ratio)\n",
    "                weighted_log_likelihood_score[i][j] = value\n",
    "    return weighted_log_likelihood, weighted_log_likelihood_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Scorers List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_scorers(weighted_log_likelihood_score):\n",
    "    top_contendants = list(zip(weighted_log_likelihood_score.max(0), weighted_log_likelihood_score.argmax(0)))\n",
    "    dtype = [('score', float), ('class', int)]\n",
    "    top_contendants = np.array(top_contendants, dtype=dtype)\n",
    "    top_contendants = np.sort(top_contendants, order='score')[::-1]\n",
    "    return top_contendants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing only the features which has a significant score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_words(top_contendants, processed_set):\n",
    "    new_processed_set = []\n",
    "    for i in range(len(processed_set)):\n",
    "        if(top_contendants[i][0] > 0.0009999):\n",
    "            new_processed_set.append(processed_set[i])\n",
    "    # print(len(new_processed_set))\n",
    "    return new_processed_set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(feature_list, new_processed_set):\n",
    "    word_frequency = []\n",
    "    for i in range(len(feature_list)):\n",
    "        word_in_sentence = [0 for row in range(len(new_processed_set))]\n",
    "        for j in range(len(feature_list[i])):\n",
    "            if(feature_list[i][j] in new_processed_set):\n",
    "                key = new_processed_set.index(feature_list[i][j])\n",
    "                word_in_sentence[key] += 1\n",
    "        word_frequency.append(word_in_sentence)\n",
    "\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_single(feature_list, new_processed_set):\n",
    "    word_frequency = []\n",
    "    word_in_sentence = [0 for row in range(len(new_processed_set))]\n",
    "    for j in range(len(feature_list)):\n",
    "        if(feature_list[j] in new_processed_set):\n",
    "            key = new_processed_set.index(feature_list[j])\n",
    "            word_in_sentence[key] += 1\n",
    "\n",
    "    return word_in_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB:\n",
    "    def __init__(self, max_iter = 1, method = \"Normal\"):\n",
    "        self.max_iter = max_iter\n",
    "        self.method = method\n",
    "        self.n_feature = 0\n",
    "        self.n_class = 0\n",
    "        self.n_train = 0\n",
    "        self.score = 0\n",
    "        self.list_class = []        \n",
    "        self.probability_class = {}\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, train_X, train_Y):      \n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.n_feature = len(train_X[0])\n",
    "        self.list_class = list(set(train_Y))\n",
    "        self.n_class = len(self.list_class)\n",
    "        self.n_train = len(train_Y)\n",
    "        \n",
    "        self.probability_class = {self.list_class[i] : train_Y.count(self.list_class[i])/self.n_train for i in range(self.n_class)}\n",
    "        \n",
    "        # Conditional Feature probabilities\n",
    "        self.probability_feature_class = [[0.0 for col in range(self.n_feature)] for row in range(self.n_class)]\n",
    "        \n",
    "        for i in range(self.n_train):\n",
    "            for j in range(self.n_feature):\n",
    "                self.probability_feature_class[self.list_class.index(train_Y[i])][j] += train_X[i][j]\n",
    "        \n",
    "        # Joint Feature Probabilities\n",
    "        for i in range(self.n_class):\n",
    "            temp = sum(self.probability_feature_class[i])\n",
    "            for j in range(self.n_feature):\n",
    "                self.probability_feature_class[i][j] /= temp\n",
    "\n",
    "        \n",
    "    def predict(self, test_X):\n",
    "        max_proximal = []\n",
    "        for i in range(self.n_class):\n",
    "            temp_prob = 1\n",
    "            for j in range(self.n_feature):\n",
    "                if(test_X[j] != 0):\n",
    "                    temp_prob *= test_X[j] * self.probability_feature_class[i][j]\n",
    "\n",
    "            max_proximal.append(temp_prob)\n",
    "\n",
    "        return self.list_class[max_proximal.index(max(max_proximal))]\n",
    "\n",
    "\n",
    "    def score_(self, test_X, test_Y):       \n",
    "        correct = 0\n",
    "        for i in range(len(test_X)):\n",
    "            if(self.predict(test_X[i]) == test_Y[i]):\n",
    "                correct += 1\n",
    "\n",
    "        self.score = correct/len(test_X)\n",
    "        return self.score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ME:\n",
    "    def __init__(self, max_iter = 1, method = \"Normal\"):\n",
    "        self.max_iter = max_iter\n",
    "        self.method = method\n",
    "        self.n_feature = 0\n",
    "        self.n_class = 0\n",
    "        self.n_train = 0\n",
    "        self.score = 0\n",
    "        self.list_class = []\n",
    "                \n",
    "    \n",
    "    def fit(self, train_X, train_Y):\n",
    "        \n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.n_feature = len(train_X[0])\n",
    "        self.list_class = list(set(train_Y))\n",
    "        self.n_class = len(self.list_class)\n",
    "        self.n_train = len(train_Y)\n",
    "        \n",
    "        # Conditional Feature probabilities\n",
    "        self.probability_feature_class = np.array([[0.0 for col in range(self.n_feature)] for row in range(self.n_class)])\n",
    "        \n",
    "        for i in range(self.n_train):\n",
    "            for j in range(self.n_feature):\n",
    "                self.probability_feature_class[self.list_class.index(train_Y[i])][j] += train_X[i][j]\n",
    "        \n",
    "        \n",
    "        feature_sum = np.sum(self.probability_feature_class, axis = 0)\n",
    "        self.sample_size = np.sum(feature_sum)\n",
    "        self.entropy = np.array([[float(\"-inf\") for col in range(self.n_feature)] for row in range(self.n_class)])\n",
    "        \n",
    "        \n",
    "        for i in range(self.n_feature):\n",
    "            feature_prb = feature_sum[i] / self.sample_size\n",
    "            for j in range(self.n_class):\n",
    "                prob_y_x = (1.0 * self.probability_feature_class[j][i]) / feature_sum[i]\n",
    "#                 if(feature_sum[i] == 0):\n",
    "#                 #print(feature_prb, prob_y_x)\n",
    "                if(prob_y_x > 0):\n",
    "                    \n",
    "                    if(prob_y_x == 1):\n",
    "                        prob_y_x = 0.99999999999\n",
    "                    \n",
    "                    self.entropy[j][i] = feature_prb * prob_y_x * math.log(prob_y_x)\n",
    "                    #print(feature_prb, feature_sum[i], self.sample_size,prob_y_x, self.entropy[j][i])\n",
    "                    #print(self.entropy[i][j])\n",
    "                    \n",
    "\n",
    "        #print(np.array(self.entropy))     \n",
    "        \n",
    "    def predict(self, test_X):\n",
    "        \n",
    "        max_proximal = [0.0 for row in range(self.n_class)]\n",
    "        \n",
    "        for j in range(self.n_feature):\n",
    "            if(test_X[j] != 0):\n",
    "                for i in range(self.n_class):\n",
    "                    if(self.entropy[i][j] != float(\"-inf\")):\n",
    "                        max_proximal[i] += (-1.0 * self.entropy[i][j])\n",
    "        \n",
    "        #print(np.array(max_proximal))\n",
    "        return self.list_class[np.argmax(max_proximal)]\n",
    "\n",
    "    \n",
    "    def score_(self, test_X, test_Y):       \n",
    "        correct = 0.0\n",
    "        for i in range(len(test_X)):\n",
    "            if(self.predict(test_X[i]) == test_Y[i]):\n",
    "                correct += 1\n",
    "        print(correct)\n",
    "        self.score = correct/len(test_X)\n",
    "        return self.score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Svc:\n",
    "    def __init__(self, max_iter = 1, method = \"Normal\"):\n",
    "        self.max_iter = max_iter\n",
    "        self.method = method\n",
    "        self.n_feature = 0\n",
    "        self.n_class = 0\n",
    "        self.n_train = 0\n",
    "        self.score = 0\n",
    "        self.list_class = []\n",
    "                \n",
    "    \n",
    "    def fit(self, train_X, train_Y):\n",
    "        \n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.n_feature = len(train_X[0])\n",
    "        self.list_class = list(set(train_Y))\n",
    "        self.n_class = len(self.list_class)\n",
    "        self.n_train = len(train_Y)\n",
    "        \n",
    "        # Linear Division\n",
    "        self.probability_feature_class = np.array([[0.0 for col in range(self.n_feature)] for row in range(self.n_class)])\n",
    "        \n",
    "        for i in range(self.n_train):\n",
    "            for j in range(self.n_feature):\n",
    "                self.probability_feature_class[self.list_class.index(train_Y[i])][j] += train_X[i][j]\n",
    "        \n",
    "        self.feature_max = np.max(self.probability_feature_class, axis = 0)       \n",
    "        self.feature_argmax = np.argmax(self.probability_feature_class, axis = 0)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def predict(self, test_X):\n",
    "        \n",
    "        max_proximal = [0.0 for row in range(self.n_class)]        \n",
    "        for i in range(self.n_feature):\n",
    "            if(test_X[i] != 0):\n",
    "                max_proximal[self.feature_argmax[i]] += test_X[i] * self.feature_max[i]\n",
    "                \n",
    "        return self.list_class[np.argmax(max_proximal)]\n",
    "\n",
    "    \n",
    "    def score_(self, test_X, test_Y):       \n",
    "        correct = 0.0\n",
    "        for i in range(len(test_X)):\n",
    "            if(self.predict(test_X[i]) == test_Y[i]):\n",
    "                correct += 1\n",
    "        print(correct)\n",
    "        self.score = correct/len(test_X)\n",
    "        return self.score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmModel(word_frequency, label_list):\n",
    "    svc = LinearSVC()\n",
    "    \n",
    "    score = []\n",
    "    combined_data = list(zip(word_frequency, label_list))\n",
    "    classifier = NB()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch\", i+1)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Shuffle the data\n",
    "        random.shuffle(combined_data)\n",
    "        word_frequency, label_list = zip(*combined_data)\n",
    "\n",
    "        train_X = word_frequency[:train_count]\n",
    "        test_X = word_frequency[train_count:train_count+test_count]\n",
    "        train_Y = label_list[:train_count]\n",
    "        test_Y = label_list[train_count:train_count + test_count]\n",
    "\n",
    "        svc.fit(train_X, train_Y)\n",
    "        score.append(round(svc.score(test_X, test_Y), 4))\n",
    "        print(\"Score\", score[i])\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"Training took: \", round(end_time - start_time,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum ENtropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropyModel(word_frequency, label_list):\n",
    "    logreg = linear_model.LogisticRegression(C = 1e5)\n",
    "    \n",
    "    score = []\n",
    "    combined_data = list(zip(word_frequency, label_list))\n",
    "    classifier = NB()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch\", i+1)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Shuffle the data\n",
    "        random.shuffle(combined_data)\n",
    "        word_frequency, label_list = zip(*combined_data)\n",
    "\n",
    "        train_X = word_frequency[:train_count]\n",
    "        test_X = word_frequency[train_count:train_count+test_count]\n",
    "        train_Y = label_list[:train_count]\n",
    "        test_Y = label_list[train_count:train_count + test_count]\n",
    "\n",
    "        logreg.fit(train_X, train_Y)\n",
    "        score.append(round(logreg.score(test_X, test_Y), 4))\n",
    "        print(\"Score\", score[i])\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"Training took: \", round(end_time - start_time,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbModel(word_frequency, label_list):\n",
    "    gnb = GaussianNB()\n",
    "    \n",
    "    score = []\n",
    "    combined_data = list(zip(word_frequency, label_list))\n",
    "    classifier = NB()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch\", i+1)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Shuffle the data\n",
    "        random.shuffle(combined_data)\n",
    "        word_frequency, label_list = zip(*combined_data)\n",
    "\n",
    "        train_X = word_frequency[:train_count]\n",
    "        test_X = word_frequency[train_count:train_count+test_count]\n",
    "        train_Y = label_list[:train_count]\n",
    "        test_Y = label_list[train_count:train_count + test_count]\n",
    "\n",
    "        gnb.fit(train_X, train_Y)\n",
    "        score.append(round(gnb.score(test_X, test_Y), 4))\n",
    "        print(\"Score\", score[i])\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"Training took: \", round(end_time - start_time,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction with GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gui_process(sentence, new_processed_set, classifier):\n",
    "    vertical_stack1 = []\n",
    "    feature_list1 = []\n",
    "    token = tokenize_sentence(sentence)\n",
    "    token = removeStopWords(token)\n",
    "    token = lemmatize_words(token)\n",
    "    token = remove_small_words(token)\n",
    "    #ngram = get_allngrams(tokens,n)\n",
    "    ngram = get_ngrams(token,n)\n",
    "    new_num = []\n",
    "    for j in range(len(ngram)):\n",
    "        ngram[j].sort()\n",
    "        vertical_stack1.extend(ngram[j])\n",
    "        new_num.extend(ngram[j])\n",
    "    feature_list1.append(new_num)\n",
    "    processed_set1 = unique_entry(vertical_stack1)\n",
    "    print(processed_set1)\n",
    "\n",
    "    if(len(processed_set1) != 0):\n",
    "        sentence_features = []\n",
    "        word_frequency = bag_of_words_single(processed_set1, new_processed_set)\n",
    "        temp_word_frequency = []\n",
    "        temp_word_frequency.append(word_frequency)\n",
    "        predicted_Y = classifier.predict(temp_word_frequency)\n",
    "        print(predicted_Y)\n",
    "        gui.show_image(predicted_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Classifier for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(word_frequency, label_list, classifier = NB()):\n",
    "    score = []\n",
    "    combined_data = list(zip(word_frequency, label_list))\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch\", i+1)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Shuffle the data\n",
    "        random.shuffle(combined_data)\n",
    "        word_frequency, label_list = zip(*combined_data)\n",
    "\n",
    "        train_X = word_frequency[:train_count]\n",
    "        test_X = word_frequency[train_count:train_count+test_count]\n",
    "        train_Y = label_list[:train_count]\n",
    "        test_Y = label_list[train_count:train_count + test_count]\n",
    "\n",
    "        classifier.fit(train_X, train_Y)\n",
    "        score.append(classifier.score_(test_X, test_Y))\n",
    "        end_time = time.time()\n",
    "        print(\"Time\", round(end_time - start_time,4), \"Score\", score[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_extraction():\n",
    "\n",
    "    # #### Calling Processing functions\n",
    "    sentence_list,label_list = read_file(\"/Users/abhianshusingla/Documents/Multi-Sentiment Analysis/data/new_data.xlsx\")\n",
    "    list_class = list(set(label_list))\n",
    "    feature_list, vertical_stack = data_processing(sentence_list,label_list)\n",
    "    #print(len(feature_list), len(vertical_stack))\n",
    "    processed_set = unique_entry(vertical_stack)\n",
    "    #print(len(processed_set), len(vertical_stack))\n",
    "    weighted_log_likelihood, weighted_log_likelihood_score = score_features(feature_list, processed_set,list_class,label_list)\n",
    "    top_contendants = top_scorers(weighted_log_likelihood_score)\n",
    "    new_processed_set = filtered_words(top_contendants, processed_set)\n",
    "    word_frequency = bag_of_words(feature_list, new_processed_set)\n",
    "    \n",
    "    print(\"Start Training\")\n",
    "    \n",
    "#     print(\"### Naive Bayes ###\")\n",
    "#     run_classifier(word_frequency, label_list, classifier = NB())\n",
    "    \n",
    "#     print(\"### Maximum Entropy ###\")\n",
    "#     run_classifier(word_frequency, label_list, classifier = ME())\n",
    "    \n",
    "#     print(\"### Support Vector Machine ###\")\n",
    "#     run_classifier(word_frequency, label_list, classifier = Svc())\n",
    "        \n",
    "#     print(\"### Standard SVM ###\")\n",
    "#     svmModel(word_frequency, label_list)\n",
    "    \n",
    "#     print(\"### Standard Entropy ###\")\n",
    "#     entropyModel(word_frequency, label_list)\n",
    "    \n",
    "#     print(\"### Standard Naive Bayes ###\")\n",
    "#     nbModel(word_frequency, label_list)\n",
    "    \n",
    "    # Training\n",
    "    train_X = word_frequency[:train_count]\n",
    "    test_X = word_frequency[train_count:train_count+test_count]\n",
    "    train_Y = label_list[:train_count]\n",
    "    test_Y = label_list[train_count:train_count + test_count]\n",
    "\n",
    "    classifier = LinearSVC()\n",
    "    classifier.fit(train_X, train_Y)\n",
    "    score = classifier.score(test_X, test_Y)\n",
    "    print(score)\n",
    "    \n",
    "#     temp = []\n",
    "#     temp.append(test_X[0])\n",
    "#     print(\"Prediction\", classifier.predict(temp))\n",
    "\n",
    "    sentence = \"\"\n",
    "    while(True):\n",
    "        new_sentence = gui.getSentence()\n",
    "        if(sentence != new_sentence):           \n",
    "            \n",
    "            sentence = new_sentence\n",
    "            gui_process(sentence, new_processed_set,classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thread Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "0.507\n",
      "['Passed', 'exam']\n",
      "['joy']\n",
      "[]\n",
      "[]\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad']\n",
      "['anger']\n",
      "['Mad', 'fri']\n",
      "['anger']\n",
      "['Mad', 'frie']\n",
      "['anger']\n",
      "['Mad', 'frien']\n",
      "['anger']\n",
      "['Mad', 'friend']\n",
      "['anger']\n",
      "[]\n",
      "[]\n",
      "['Say']\n",
      "['joy']\n",
      "['Sayi']\n",
      "['joy']\n",
      "['Sayin']\n",
      "['joy']\n",
      "['Saying']\n",
      "['joy']\n",
      "['Saying']\n",
      "['joy']\n",
      "['Saying']\n",
      "['joy']\n",
      "['Saying']\n",
      "['joy']\n",
      "['Saying', 'goo']\n",
      "['joy']\n",
      "['Saying', 'good']\n",
      "['joy']\n",
      "['Saying', 'good-']\n",
      "['joy']\n",
      "['Saying', 'good-b']\n",
      "['joy']\n",
      "['Saying', 'good-by']\n",
      "['joy']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye']\n",
      "['sadness']\n",
      "['Saying', 'good-bye', 'rel']\n",
      "['sadness']\n",
      "['Saying', 'good-bye', 'rela']\n",
      "['sadness']\n",
      "['Saying', 'good-bye', 'relat']\n",
      "['sadness']\n",
      "['Saying', 'good-bye', 'relati']\n",
      "['sadness']\n",
      "['Saying', 'good-bye', 'relativ']\n",
      "['sadness']\n",
      "['Saying', 'good-bye', 'relative']\n",
      "['sadness']\n",
      "[]\n",
      "[]\n",
      "['Acc']\n",
      "['joy']\n",
      "['Acci']\n",
      "['joy']\n",
      "['Accid']\n",
      "['joy']\n",
      "['Accide']\n",
      "['joy']\n",
      "['Acciden']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident']\n",
      "['joy']\n",
      "['Accident', 'car']\n",
      "['fear']\n",
      "['car']\n",
      "['fear']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['car']\n",
      "['fear']\n",
      "['car']\n",
      "['fear']\n",
      "['car']\n",
      "['fear']\n",
      "['car']\n",
      "['fear']\n",
      "['car', 'acc']\n",
      "['fear']\n",
      "['car', 'acci']\n",
      "['fear']\n",
      "['car', 'accid']\n",
      "['fear']\n",
      "['car', 'accide']\n",
      "['fear']\n",
      "['car', 'acciden']\n",
      "['fear']\n",
      "['car', 'accident']\n",
      "['fear']\n",
      "[]\n",
      "[]\n",
      "['Tel']\n",
      "['joy']\n",
      "['Tell']\n",
      "['joy']\n",
      "['Telli']\n",
      "['joy']\n",
      "['Tellin']\n",
      "['joy']\n",
      "['Telling']\n",
      "['joy']\n",
      "['Telling']\n",
      "['joy']\n",
      "['Telling']\n",
      "['joy']\n",
      "['Telling']\n",
      "['joy']\n",
      "['Telling']\n",
      "['joy']\n",
      "['Telling']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie']\n",
      "['joy']\n",
      "['Telling', 'lie', 'fri']\n",
      "['joy']\n",
      "['Telling', 'lie', 'frie']\n",
      "['joy']\n",
      "['Telling', 'lie', 'frien']\n",
      "['joy']\n",
      "['Telling', 'lie', 'friend']\n",
      "['disgust']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\"n't\"]\n",
      "['joy']\n",
      "[\"n't\"]\n",
      "['joy']\n",
      "[\"n't\"]\n",
      "['joy']\n",
      "[\"n't\"]\n",
      "['joy']\n",
      "[\"n't\", 'spe']\n",
      "['joy']\n",
      "[\"n't\", 'spea']\n",
      "['joy']\n",
      "[\"n't\", 'speak']\n",
      "['shame']\n",
      "[\"n't\", 'speak']\n",
      "['shame']\n",
      "[\"n't\", 'speak']\n",
      "['shame']\n",
      "[\"n't\", 'speak']\n",
      "['shame']\n",
      "[\"n't\", 'speak']\n",
      "['shame']\n",
      "[\"n't\", 'speak']\n",
      "['shame']\n",
      "[\"n't\", 'speak']\n",
      "['shame']\n",
      "[\"n't\", 'speak']\n",
      "['shame']\n",
      "[\"n't\", 'speak', 'tru']\n",
      "['shame']\n",
      "[\"n't\", 'speak', 'trut']\n",
      "['shame']\n",
      "[\"n't\", 'speak', 'truth']\n",
      "['shame']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abhianshusingla/anaconda3/lib/python3.6/tkinter/__init__.py\", line 1699, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/Users/abhianshusingla/anaconda3/lib/python3.6/tkinter/__init__.py\", line 2057, in destroy\n",
      "    Misc.destroy(self)\n",
      "  File \"/Users/abhianshusingla/anaconda3/lib/python3.6/tkinter/__init__.py\", line 589, in destroy\n",
      "    self.tk.deletecommand(name)\n",
      "_tkinter.TclError: can't delete Tcl command\n"
     ]
    }
   ],
   "source": [
    "# Making the Threads for communicating with the grid-world map\n",
    "Sentiment_thread = threading.Thread(target = sentiment_extraction)\n",
    "Sentiment_thread.daemon = True\n",
    "Sentiment_thread.start()\n",
    "\n",
    "# Start GUI\n",
    "gui.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
